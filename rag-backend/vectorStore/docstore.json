[[["doc-0",{"pageContent":"AI driven Test Case Generation\nAn In-Depth study on the Utilization of Large Language Models for Test Case\nGeneration\nNicole Johnsson\nNicole Johnsson\nSpring 2023\nDegree Project in Interaction Technology and Design, 30 credits\nSupervisor: Gustav Grund Pihlgren\nExternal Supervisor: Bharath Karthikeyan\nExaminer: Ola Ringdahl\nMaster of Science Programme in Interaction Technology and Design, 300 credits\ni","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":1,"lines":{"from":1,"to":12}}}}],["doc-1",{"pageContent":"Abstract\nThis  study  investigates  the  utilization  of  Large  Language  Models  for\nTest Case Generation.  The study uses the Large Language model and\nEmbedding model provided by Llama, specifically Llama2 of size 7B,\nto generate test cases given a defined input.  The study involves an im-\nplementation that uses customization techniques called Retrieval Aug-\nmented Generation (RAG) and Prompt Engineering.  RAG is a method\nthat in this study, stores organisation information locally, which is used\nto  create  test  cases.   This  stored  data  is  used  as  complementary  data\napart  from  the  pre-trained  data  that  the  large  language  model  has  al-\nready trained on.  By using this method, the implementation can gather\nspecific organisation data and therefore have a greater understanding of\nthe required domains.  The objective of the study is to investigate how\nAI-driven test case generation impacts the overall software quality and\ndevelopment efficiency. This is evaluated by comparing the output of the\nAI-based system, to manually created test cases, as this is the company\nstandard at the time of the study. The AI-driven test cases are analyzed\nmainly in the form of coverage and time, meaning that we compare to\nwhich  degree  the  AI  system  can  generate  test  cases  compared  to  the\nmanually created test case. Likewise, time is taken into consideration to\nunderstand how the development efficiency is affected.","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":2,"lines":{"from":1,"to":21}}}}],["doc-2",{"pageContent":"The  results  reveal  that  by  using  Retrieval  Augmented  Generation\nin combination with Prompt Engineering, the system is able to identify\ntest cases to a certain degree. The results show that 66.67% of a specific\nproject was identified using the AI, however, minor noise could appear\nand results might differ depending on the project’s complexity. Overall\nthe results revealed how the system can positively impact the develop-\nment efficiency and could also be argued to have a positive effect on\nthe  software  quality.   However,  it  is  important  to  understand  that  the\nimplementation as its current stage, is not sufficient enough to be used\nindependently,  but should rather be used as a tool to more efficiently\ncreate test cases.\nii","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":2,"lines":{"from":22,"to":33}}}}],["doc-3",{"pageContent":"Acknowledgements\nI would like to thank the people around me, who believed in my abilities, in moments when\nI had a drought.  To my parents that kept encouraging me, and reminding me that no step,\neasy or hard, is impossible to make.  That a star is only one step from the moon.  To my\nbrother that every day inspired me to be better and have a hunger for learning.  And lastly,\nto my family around the world, who celebrated all of my successes like their own.\nI would also like to express my gratitude to my supervisor from Ume\n ̊\na University, Gus-\ntav Grund Pihlgren, who continuously provided guidance, calmness, and advice that was\nneeded. To my external supervisor, Bharath Karthikeyan, who with a direction and a smile\nhelped me through the entire process. To my teams, Onboarding Experience and Activation\nExperience, and to the organization PayPal as a whole.  Thank you for listening, cheering,\nand lifting me up.\nLast but definitely not least, I want to thank my friends.  That even on the rainiest days\nbrought me sunlight.\niii","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":3,"lines":{"from":1,"to":17}}}}],["doc-4",{"pageContent":"Contents\n1    Introduction1\n1.1Quality Assurance in Software Development2\n1.2Objective3\n2    Background5\n2.1Large Language Models5\n2.1.1Embedding5\n2.1.2Transformer Architecture6\n2.1.3Scaling Laws8\n2.2Customization techniques9\n2.2.1Prompt engineering9\n2.2.2Retrieval Augmented Generation10\n2.3Testing Method12\n2.3.1White box testing12\n3    Method14\n3.1Identify and Define14\n3.1.1Interviews14\n3.1.2Persona15\n3.2Research and plan15\n3.2.1Literature study16\n3.2.2Exploring the company resources16\n3.3Produce and Implement16\n3.3.1Create Knowledge base17\n3.3.2Overview of the system18\n3.3.3Prompt Template and Llama parameters19\n3.4Test and Evaluate20\n4    Results22\n4.1Interviews22\n4.1.1Persona24\n4.2Evaluate the system24\n5    Discussion28\niv","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":4,"lines":{"from":1,"to":32}}}}],["doc-5",{"pageContent":"5.1Difficulties and solutions30\n6    Conclusion32\n6.1Future work32\nReferences35\nA   Interview Questions40\nv","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":5,"lines":{"from":1,"to":6}}}}],["doc-6",{"pageContent":"1   Introduction\nArtificial Intelligence, or AI is a type of technology that mimics intelligent behavior through\nthe use of computer models. The term was officially introduced in 1955, by John McCarthy\nbut is today fully incorporated into society through numerous technologies such as the Voice\nAssistant Siri or Fraud Detection in banks [34].\nOne of the most acknowledged breakthroughs in the field of AI is ChatGTP, a chatbot\nthat uses Natural Language Processing (NLP) to enable human-like conversation dialogues.\nNLP can be described as a field of Computer Science and Artificial Intelligence that fo-\ncuses on enabling computers to understand, interpret, and generate human language.  The\nfield aims to enable machines to perform tasks that traditionally require a human-level un-\nderstanding of languages. These types of tasks are for example text summarizing and speech\nrecognition, among others [38] [43].\nEven if Artificial Intelligence is a commonly used term,  the term itself encompasses\nvarious underlying layers such as Machine Learning and Deep Learning.  Machine learn-\ning or ML, in comparison to Artificial intelligence, does not only mimic the human way of\nthinking but instead uses data-trained models to perform different tasks [9]. This discipline\nof AI focuses on building computers that improve automatically through experience.  Ma-\nchine Learning is a heavily researched topic that rapidly has been evolving and is today used","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":6,"lines":{"from":1,"to":18}}}}],["doc-7",{"pageContent":"for evidence-based decision-making [18].  Figure 1 illustrates the different layers included\nunder the field of AI.\nFigure 1:Illustration of the layers of AI [56]\nContinuing on Figure 1, the next layer after Machine learning is Deep learning.  Deep\nlearning is a subset that uses Artificial Neural Networks (ANNs) with multiple layers. These\nNeural Networks aim to mimic the behavior of the human brain [16].  Initially, Machine\nLearning faced difficulties in limitations with processing natural raw data which caused the\n1","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":6,"lines":{"from":19,"to":26}}}}],["doc-8",{"pageContent":"development of Deep Learning. The underlying Artificial Neural Network (ANN) is a deep\nlearning method that was inspired by the concept of the human brain [10]. The similarities\nbetween the Artificial and Biological Neural Network, are clear since the ANN also includes\nnodes that behave similarly as neurons in our brain. These nodes are organized into layers,\nwhich is where the information flows through. The connection strength between the nodes\nis adjusted during training to learn from data, which is essential to recognize patterns and\nmake predictions [25][44].\nLastly from the illustration, the final layer is Generative AI. Artificial Intelligence Gen-\nerated Content or AIGC is a type of technology that creates different types of content by\nusing Generative AI, which for example is seen with ChatGTP which creates text in differ-\nent forms, but also DALL-E which creates images based on prompts [6]. From a technical\npoint of view, AIGC extracts human instructions to generate content satisfying the instruc-\ntions.   The AIGC technology has advanced tremendously by training more sophisticated\ngenerative models on larger datasets[6].\nOne of the specific large models in deep learning is the Large Language Model (LLM).\nThese models are pre-trained on colossal amounts of data.  Underlying the model is a set\nof Neural Networks,  which act as transformers in the system.  The networks as a whole\nconsist of an encoder and a decoder with self-attention capabilities.  Why Large Language","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":7,"lines":{"from":1,"to":18}}}}],["doc-9",{"pageContent":"Models have an important role in today’s technological evolution,  can partly be because\nof the models’ flexibility [2].  As previously described with Natural Language Processing,\nLLMs are a subset of NLP and can therefore perform such tasks.  However, since LLMs\nhave trained on significantly bigger datasets, these models also tend to perform significantly\nbetter. LLMs can be described as an advancement of the NLP capabilities, that not only can\nproduce a summary like NLP, but also can produce, manipulate, and comprehend text in a\nhuman-like way, due to the underlying neural network of LLMs [46].\nBecause of this, LLMs have shown great potential in solving problems and potential to\ndisrupt content creation, as described with Artificial Intelligence Generated Content. Some\nother areas where LLMs have had a great impact is in the way people use search engines and\nvirtual assistants [2]. ChatGTP as mentioned previously is one of the products that utilizes\nLLMs to solve tasks such as summarizing text, answering questions, creating content such\nas presentations, and so on.\nOther companies that have utilized Large Language Models to solve other specific prob-\nlems are for example Google with the underlying LLM called PaLM or Meta with LLM\ncalled Llama.  Google Duet, launched by Google, is among other features, a Code Assis-\ntant, designed to facilitate different tasks such as generating code blocks based on comments","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":7,"lines":{"from":19,"to":35}}}}],["doc-10",{"pageContent":"[11].  This has been widely applied to for example create Unit Tests, with a large research\nblock investigating this specific use case.  One part of Software Development that is con-\nsidered to be resource-intensive, and could therefore benefit from automation with LLMs is\nQuality Assurance.\n1.1    Quality Assurance in Software Development\nA significant part of developing software is the ability to test the software quality.   The\nassigned role for this is called Quality Assurance Engineer and is a role with many respon-\nsibilities in the development process.  As a Quality Assurance Engineer, also called a QA,\nthe role often acts as a bridge between stakeholders and developers, interacting with various\n2","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":7,"lines":{"from":36,"to":45}}}}],["doc-11",{"pageContent":"development team members [1]. Commonly the following phases are used.\n•Designing  the  tests:  The  QA  checks  in  each  stage  that  the  product  meets  all  the\nrequirements in both expected and unexpected scenarios. To check this, QA engineers\ndesign tests to identify issues that could appear.\n•Executing Tests: Running the designed tests and documenting the results.\n•Analyzing Tests:  Analyze the results of the unexpected outcomes and ascertain if\nany areas have not been assessed.\n•Enhancing Tests: As the testing cycle continues, QA engineers modify and redesign\ntests to improve quality depending on new findings.\nThere are numerous scenarios that a product needs to be tested against to ensure that\nthe product being delivered to the users holds the standard that the organization has defined.\nThese scenarios are tested through many different methodologies that address the testing\nphases that were previously described.  The first testing phase meaning the test design is a\nprocess that defines how testing has to be done.  This process involves the steps of iden-\ntifying the testing techniques, test scenarios, test cases, test data, and expected test results\n[49].\nBecause of the amount of steps included in this first testing phase, the phase can there-\nfore be time-consuming for the Quality Assurance Engineers. The specific part of creating\nTest Cases requires the QA engineer to think of all of the specific scenarios that the user can","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":8,"lines":{"from":1,"to":19}}}}],["doc-12",{"pageContent":"face. If the product is less complex, these steps might not require as much time to identify.\nBut as soon as the product complexity is higher, this also means that the identification of\nscenarios might be more difficult to map.  For this reason, the exploration of using Large\nLanguage Models in order to map user scenarios in the form of Test Cases is interesting to\nincrease QA productivity.\nEven if the exploration of using Generative AI in the context of software testing has\nbeen gaining more traction, the specific use case of using Generative AI for facilitating the\nTest Planning phase is still very limited.  As QA needs to test the software against many\nthreats and scenarios, it can be considered a complex problem the AI algorithm needs to\nface. In this study, Generative AI in the context of Software Testing is investigated to solve\nthe specific task of generating Test Cases with Large Language Models.\n1.2    Objective\nThis study is an early phase exploratory study, that investigates the potential of using Large\nLanguage Models to automatically create Test Cases.  The study aims to implement and\nevaluate:\n•  A system that simplifies the Test Design phase by creating automatic test cases by\nusing Large Language Models.\nThe objective of the study is to use Generative AI to enhance the test case design pro-\ncess by creating or complementing the existing test cases designed by Quality Assurance\nEngineers. The following research question will be answered through the project:\n3","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":8,"lines":{"from":20,"to":40}}}}],["doc-13",{"pageContent":"•  How does the integration of AI-driven test case generation in software development\npipelines impact the overall software quality and development efficiency?\n4","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":9,"lines":{"from":1,"to":3}}}}],["doc-14",{"pageContent":"2   Background\nThis background chapter is divided into three main parts.   The first section introduces a\nbreakdown of Large Language Models and important terminology.  The concept of LLMs\nis broken down into sections discussing Embedding, Transformer Architecture, and Scaling\nLaws.  The second part of the chapter addresses the commonly used customization tech-\nniques for Large Language Models.  The relevant customization techniques are discussed,\nnamely Retrieval Augmented Generation (RAG) and Prompt Engineering.  The third and\nfinal part of this chapter addresses the concept of Software Testing.  The chapter aims to\nintroduce the reader to the methodologies used when testing and specifically break down\nthe concept of White Box Testing.\n2.1    Large Language Models\nLarge Language Models, as described in the previous chapter, are a type of Artificial In-\ntelligence that uses Deep Learning techniques in combination with large data sets, in order\nto solve diverse tasks.  As a concept, LLMs are not a new concept but rather have evolved\nwith Artificial Intelligence.  From an evolutionary point of view, earlier language models\nhad a greater focus on generating text data, while newer models like Google’s PaLM focus\non complex task-solving [64] [7].\n2.1.1    Embedding\nOne commonly used concept when discussing Large Language Models (LLMs) and\nNatural Language Processing (NLP) is Embedding. Humans unlike computers can interpret","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":10,"lines":{"from":1,"to":20}}}}],["doc-15",{"pageContent":"different  nuances  in  a  language.   From  the  human  brain,  we  make  connections  such  as\nunderstanding that a king is also a man, without it needing to be clearly stated in the text.\nBut from a computer’s point of view, the language or data, needed to be understood must\nbe represented as numbers. These numbers are illustrated as a point in a vector space. The\nprocess of converting such data into numbers is a process called Embedding, and is a heavily\nresearched topic due to its essential role in data analysis [58] [15].\nThe  following  Figure  2,  illustrates  four  points  representing  the  words  man,  woman,\nking, and queen. Depending on the word’s numerical illustration we can make the connec-\ntion that a king and a man are related, similarly to a woman and a queen.\nFollowing the previous example, the human brain is able to understand that a King also\nis a Man, or that Rome is a city in Italy, which is essential in order to obtain the desired\ninformation. If the word would directly be encoded, this would also mean that the computer\nwould not be able to connect the words in the same way as the human brain does. In order\nto practically solve this issueWord Embeddingdoes not only identify the word but also\nsuccessfully identifies the semantics and syntax of the word to build a vector representation\nof this information.  The main objective is to capture the characteristics of the text rather\n5","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":10,"lines":{"from":21,"to":37}}}}],["doc-16",{"pageContent":"Figure 2:Illustration word embedding [15]\nthan obtaining just the features as numbers [24] [26].\nA different approach to embedding, in comparison to Word embedding, isContextual\nembedding. This technique shifts focus from word representation to giving a contextual rep-\nresentation that can capture many syntactic and semantic properties of words under diverse\nlinguistic contexts.  This type of embedding technique has through different studies shown\noutstanding results in a range of commonly applied NLP tasks, such as text classification\nand text summarization [29].\nAs previously described, contextual embedding aims to capture the overall sentiment\nand syntax of the entire sentence. For example can the word ’Mouse’ have different mean-\nings depending on the context, such as a computer mouse or an animal called mouse [53].\nIf  we  would  consider  a  text  corpus  being  represented  as  an  input  sequenceSof  tokens\n(t\n1\n,t\n2\n,...,t\nN\n), then contextual embedding could be represented ast\ni\nassociating each token\nthat is a function ofS, meaningh\nt\ni\n=f(e\nt\n1\n,e\nt\n2\n,...,e\nt\nN\n).  Each of the input tokens is repre-\nsented ast\nj\nwhich is firstly mapped in a non-contextual representatione\nt\nj\n, before applying\nan aggregation functionf[30] [39].\n2.1.2    Transformer Architecture\nTechnically, when referring to the term LLMs, this is typically a referral to Transformer\nlanguage models, with hundreds of billions of parameters.  These Transformers are trained","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":11,"lines":{"from":1,"to":44}}}}],["doc-17",{"pageContent":"on huge amounts of text data, which is used for example in in PaLM, LLama, and GTP-4.\nIn the following section, the Transformer architecture is described to better understand the\nbuilding block of all LLMs [54] [12] [35].  Figure 3 gives an overview of the architecture\nwith its seven important components.\nThe first component of the system isInput and input embedding.  This component is\nwhere the user provides all input tokens. In other words, this means the query or instructions\nprovided to the LLM. But even if the user writes instructions to the model in text format, the\nmodel itself can’t understand text the way humans do. Because of this, the input needs to be\nembedded, or in other words converted into numbers, so that the computer can understand\nit. To understand more on how embedding works, read section 2.1.1.\nThe second component of the system isPositional Encoding.  This component focuses\non understanding the order of words that build up a sentence.  The order of the words will\nplay a crucial role in understanding the sentence’s meaning and can be obtained through\npositional encoding. When each word has been encoded, these numbers can be fed into the\n6","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":11,"lines":{"from":45,"to":59}}}}],["doc-18",{"pageContent":"Transformer model, along with the input embeddings. If the transformer architecture is able\nto obtain both, the positional encoded and input embedding, the LLM can more effectively\nunderstand a sentence and generate grammatically correct and semantically meaningful out-\nput.\nFigure 3:Overview of a basic transformer architecture [54]\nThe third component of the system isEncoder. This component is a part of the Neural\nNetwork that processes the input text and generates a series of hidden states that capture the\nmeaning and context of the text. The component works as a tokenizer, where the input text\nis split into a sequence of tokens. From this point, a series of self-attention layers is applied,\nmeaning that the component generates a series of hidden states that represent the input text\nat different levels of abstraction [54].\nThe fourth component of the system isOutputs (shifted right): This component predicts\nthe  next  word  in  a  sequence  by  looking  at  the  words  before  it,  which  is  possible  from\nthe huge amount of training data.  Technically the component works by moving the output\nsequence over one spot to the right, so the decoder can only use the previous words to gather\nan understanding on what the next word is.\nThe fifth component of the system isOutput Embeddings: Similarly as described in the\nfirst component, the output needs to be converted in order to be understood by the computer.","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":12,"lines":{"from":1,"to":18}}}}],["doc-19",{"pageContent":"Positional encoding is also applied at this stage.  Lastly, a loss function is used in machine\nlearning, which measures the difference between a model’s predictions and the actual target\nvalues.   This function adjusts some parts of the model to improve accuracy by reducing\nthe difference between predictions and targets to improve the overall performance.  This\nis used during two scenarios, training and inference.  When training, the loss function is\ncomputed in order to update the model parameters.  During during interference, the output\ntext is produced by correlating the model’s predicted probabilities for each token with the\n7","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":12,"lines":{"from":19,"to":26}}}}],["doc-20",{"pageContent":"corresponding token in the vocabulary.\nThe sixth component of the system isDecoder: The input representation that has been\npositionally encoded, and the positionally encoded output embeddings need to at this stage\ngo through the decoder. In transformers, multiple layers of decoders are used to generate the\noutput sequence based on the encoded input sequence. In the stages of training, the decoder\ncomponent learns how to predict the next word by looking at the words before it, similar\nto the fourth component.  In for example ChatGTP, the decoder uses natural language text\nbased on the input sequence and the context learned by the encoder to generate the output.\nThe seventh and final component of the system isLinear Layer and Softmax:  Finally\nafter all other components have been applied, the last step is to transform the output em-\nbeddings into the original input space.  The linear layer works by mapping the output em-\nbeddings to a higher-dimensional space.  Lastly, the softmax function is used to generate\na probability distribution for each output token in the vocabulary, enabling us to generate\noutput tokens with probabilities.\n2.1.3    Scaling Laws\nAs previously expressed, LLMs can be described as an advancement of the NLP capa-\nbilities, which is due to LLMs training on increasingly higher amounts of data.  However\nLarge Language Models does not only extend the data size, but also the model size and total","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":13,"lines":{"from":1,"to":18}}}}],["doc-21",{"pageContent":"compute. From different research papers, it has been demonstrated that scaling can directly,\nand positively improve the model capacity of LLMs [64] [60].  For Transformer language\nmodels, the scaling law can be represented through theKM scaling law, which firstly was\nproposed by a team at OpenAI (2020) [19]. The team proposed that the model performance\nwill be dependent on Model size (N), Data size (D) and Training compute (C). Given the\ncompute budgetc, the scaling formulas are presented underneath, whereL(·)symbolises\nthe cross entropy loss in nats [64].\nL(N) =\n\u0000\nN\nc\nN\n\u0001\nα\nN\n,α\nN\n∼0.076,N\nc\n∼8.8x10\n13\n(2.1)\nL(D) =\n\u0000\nD\nc\nD\n\u0001\nα\nD\n,α\nD\n∼0.095,D\nc\n∼5.4x10\n13\n(2.2)\nL(C) =\n\u0000\nC\nc\nC\n\u0001\nα\nC\n,α\nC\n∼0.050,C\nc\n∼3.1x10\n8\n(2.3)\nIn the same study by OpenAI, the three laws were derived by fitting the model perfor-\nmance with different values on the parameters Model size (N), Data size (D) and Training\ncompute (C), under some assumptions. The results showed that the model performance has\na strong dependence relation on the three factors.\nThe scaling law is an important part of understanding LLM behavior and can in practice\nbe used to instruct the training of LLM. As these laws have demonstrated their effective-\nness in providing a qualitative performance estimation for larger models, their application to\nsmaller models becomes particularly advantageous, notably in terms of time efficiency, en-\nabling reliable estimations. Using large-scale models can be very time-consuming and also","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":13,"lines":{"from":19,"to":79}}}}],["doc-22",{"pageContent":"involve issues such as training loss spikes.  Instead Scaling law can be used to monitor the\ntraining status of LLMs, and overall the previous studies show a tendency for performance\nincrease when increasing Model size (N), Data size (D), or Training compute (C).\n8","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":13,"lines":{"from":80,"to":83}}}}],["doc-23",{"pageContent":"Predicting on a task level\nThe majority of the research specifically scaling laws, has been done on language model-\ning.  However, in this study, the performance of Large Language Models on a task level is\nthe more interesting part.  According to a study on LLMs, a model with smaller language\nmodeling loss, usually tents to perform better on downstream tasks [64].  GPT-4 is one of\nthe models that have been reported to have some accurate predictions via scaling law, but\nthe readers have also been notified that a direct decrease in language modeling law does not\nalways indicate an improvement in model performance on such tasks.  In some cases, lan-\nguage modeling can obtain worse results when loss decreases, however, the study suggests\nthat task-level scaling law is more difficult to characterize due to it being more dependent\non the task-related information.\n2.2    Customization techniques\nLarge Language Models, as described, are showing tendencies of great potential. However,\nif  a  user  wants  to  obtain  the  maximum  results,  numerous  articles  suggest  that  a  certain\ntype of customization is needed. In a general matter, four basic layers of customization are\ncommonly applied to higher the complexity of the LLM behavior.  These layers include:\nIntegrate with Pre-Build Components, Prompt Engineering, Internal Knowledge, and Fine\nTuning [13].\nIn this project,  the two relevant layers of customization are Prompt Engineering and","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":14,"lines":{"from":1,"to":19}}}}],["doc-24",{"pageContent":"Internal Knowledge. In the following subsections, the concepts and technology of the cus-\ntomization layers are discussed.\n2.2.1    Prompt engineering\nThe first layer of customization is Prompt Engineering, meaning the process of design-\ning, refining, and optimizing the prompts or queries given to a language model to obtain\nspecific responses [28].  Generally, this is a process that is highly involved in NLP but is\nmostly correlated with interaction with Large Language Models, such as GTP-4. This disci-\npline is still considered to be relatively new, though the method is used to better understand\nthe capabilities and limitations of LLMs [41].\nWhy Prompt Engineering plays a critical role, is because of its ability to tailor the spe-\ncific behavior of the model to certain tasks and applications. The initial step of the process\nis to create a prompt, also called the input query.  This query will be provided to the LLM\nand can be practically provided in different forms such as a question, an instruction, or a\npartial sentence. A well-defined prompt should provide enough information for the model to\nunderstand the user’s intent[48]. How to effectually conduct the prompt has been discussed\nand can be broken down into a couple of components [45]. In Table 1, the commonly used\nPrompt Engineering component is shown, as well as a general description and importance\nlevel.\n9","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":14,"lines":{"from":20,"to":38}}}}],["doc-25",{"pageContent":"Table 1:Components to include when conducting a prompt, sorted from most important to\nleast important.\nPrompt Engineering\nComponent nameDescriptionImportance level\nTaskWhat  you  want  the  LLM\nto  do.    Start  the  sentence\nwith an Action word,  e.g.,\nWrite,   Generate  or  Give.\nGive me 3 healthy recipes\nMandatory\nContextGives  the  LLM  a  context\nto the question.I am a 35\nyear old female\nImportant\nExemplarHelps  understand  the  rea-\nsoning  process.Use  the\nSTAR  answer  framework:\nSituation, Task, Action, and\nResults\nImportant.    Not  necessary\nfor every prompt\nPersonaGives context to the model\nabout  how  it  should  be-\nhave.You  are  an  hiring\nmanager\nNice to have but not needed\nFormatDefine  the  output  format\nneeded.Give   me   the\nrecipes in a table format\nNice to have but not needed\nToneDefine  the  tone  of  the  an-\nswer.Casual,  formal,  or\nenthusiastic tone.\nOptional\n2.2.2    Retrieval Augmented Generation\nRetrieval Augmented Generation, also called RAG is, similarly to prompt engineering, a\nway for customization in the context of Large Language Models. In the context of retrieving\ninformation from LLMs, the most commonly used methodology is to communicate thought\ninstructive prompts in some form of chat context.   However,  when a user interacts with\nthe language model, they’re essentially extracting information from the model’s pre-trained\ndata.  Meaning that the user can only get an answer related to the data that the model has","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":15,"lines":{"from":1,"to":41}}}}],["doc-26",{"pageContent":"priory trained on.  For that reason, it is common that large language models are not able to\nanswer questions related to for example news events that have recently happened.\nBecause of this, the base models that are trained only on pre-trained data, include lim-\nitations in knowledge.  One of the downsides of explicitly using such a model is:  that the\nmodel cannot easily expand or revise their memory, can’t straightforwardly provide insight\ninto their predictions, and may produce “hallucinations” [27] [17].\nRetrieval Augmented generation is a hybrid model that combines both paramedic mem-\nory (pre-trained data) with non-paramedic memory (retrieval-based memories).  The usage\nof this type of hybrid model has been showing positive results since its ability to directly\nrevise, expand, and access knowledge [17].  A practical example of this is:  When a user\ninteracts with an LLM similar to chatGTP, the user is able to ask a lot of different ques-\n10","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":15,"lines":{"from":42,"to":53}}}}],["doc-27",{"pageContent":"tions.  Logical and historical questions are just some of the examples that the model could\nanswer if the user queries a question.  But as soon as the same user would ask a specific\nquestion like: ”When is my friend Lisa’s birthday?”, the LLM would not be able to provide\nan answer.  This is because the answer related to that question has not been found in the\npre-trained data.  In order to answer that question, the model would need some additional\ninformation. Which can be provided through this non-paramedic memory [27].\nTasks that are considered knowledge-intensive tasks can use the method called Retrieval\nAugmented Generation,  or RAG. Originally the method was introduced by Meta AI re-\nsearchers, who argued that such technology can customize large models without needing\nretraining of the entire model [42].\nPractically the technology works by taking an input, being the query of the user, and\nusing this query to extract relevant documents from a source. The input query is embedded,\nmeaning that it is,  therefore,  a number symbolizing a point in a vector space.  The doc-\numents are also embedded similarly and are symbolized as individual points in the same\nvector space as the input query.  The degree of similarity between the input query and the\ndocuments can therefore be decided by extracting the points with the shortest distance from\nthe input query point.  The amount of documents to be considered can be specified as well","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":16,"lines":{"from":1,"to":17}}}}],["doc-28",{"pageContent":"as the minimum degree of similarity.  The selected documents are concatenated as context\ntogether with the original input prompt, which together will be introduced to the text gen-\nerator, which will produce the final output combining both the pre-trained data and relevant\ninformation from the knowledge base. Figure 4, describes an overview of the method.\nFigure 4:Illustration of Retrieval Augmented Generation [27]\nAs described in the previous paragraph, Retrieval Augmented Generation works by ex-\ntracting relevant data to the input query.  In the last paragraph, documents were described,\nbut similarly, the model can extract relevant data on tokens or chucks.  The model that is\nrelevant for this study is calledRAG−tokenand allows the generator to choose content\nfrom several documents since the similarity depends on tokens instead of whole documents\n[27]. Formally, theRAG−tokenmodel is defined with the following equation:\n11","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":16,"lines":{"from":18,"to":29}}}}],["doc-29",{"pageContent":"2.3    Testing Method\nIn Software Development, Software Testing is a process aimed to evaluate and verify that\nthe  developed  system  is  behaving  as  expected.   As  an  essential  part  of  assuring  quality\nin  products,  this  also  required  a  variety  of  tests  to  be  used  that  can  mimic  the  different\nscenarios that the software can face. Unit testing, Integration testing, functional testing, and\nperformance testing are just a few of the tests that are commonly required before realising a\nnew product [23]. Among the commonly used techniques in Software Testing is White Box\nTesting.\n2.3.1    White box testing\nWhite Box Testing is a technique that mainly focuses on testing the structure of the ap-\nplication, meaning the code structure, logic, and flow [21]. The method practically uses the\ncontrol structure to create a Test Case Design, which is created to map and prevent imple-\nmentation errors that could appear in the application. Test Case Design as first introduced in\nsection 1.1, focuses on checking that each stage of the products meets all the requirements\nin both expected and unexpected scenarios.\nThe method of using White Box Testing is applicable on multiple layers of the Soft-\nware Testing process, including Integration, Unit, and System levels. The testing technique\nrequires source code access and understanding of the source code since this is the base of\nthe process.  Some of the testing techniques used in white box testing include control flow","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":17,"lines":{"from":1,"to":19}}}}],["doc-30",{"pageContent":"testing, path testing, data flow testing, loop testing, and branch coverage testing [55] [23].\nFigure 5 illustrates the different types of white box testing.  These can be discussed more\nin-depth but from the context of the study, will only be mentioned [57].\nFigure 5:Illustration of different types of White box testing\nIn order to understand the study, the practical working process of White Box Testing\nneeds to be understood. The general approach to performing White Box Testing is to:\n1.  Study the source code\n2.  Find Test Cases and execute\nEven if the first point of the general approach is fairly straightforward, the second point\n12","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":17,"lines":{"from":20,"to":29}}}}],["doc-31",{"pageContent":"follows a more generic framework.  To obtain this, the following procedure is applied [20]\n[57]:\n1.Input:A  List  of  requirements,  functional  specifications,  design  documents,  and\nsource code.\n2.Processing:Perform risk analysis, meaning identifying and analyzing potential fu-\nture events that may adversely impact a company.\n3.Test Planning:Design test case, meaning map which specific actions are required to\nverify a specific feature or functionality in the software. This is presented as detailed\nsteps, data, prerequisites, and post conditions necessary to verify a feature [5].\n4.Output:A final, often manually written.\nThe traditional way of creating test cases is to manually write the different scenarios as\ntext, which describes all possible actions that are required for a step. These steps shall also\ndiscover all the different possibilities a system could face.  If the application or product is\nlarge in size this causes problems of complexity, since the tester needs to understand and\nidentify all possible paths.  This type of testing is also considered highly expensive since\neach redesign of code or rewriting of code requires a new set of test cases.  Another no-\nticeable disadvantage is that the tester requires a high level of expertise,  which not only\ncontributes to the expenses being high but also elongates the knowledge gap between dif-\nferent domains in a software team.\n13","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":18,"lines":{"from":1,"to":20}}}}],["doc-32",{"pageContent":"3   Method\nThe following chapter contains the detailed methodology for conducting the experiment. In\norder to achieve the desired results, the methodology used follows a structure equivalent to\nthe Design Thinking process. Design Thinking as a concept can be described as an iterative,\nnon-linear process of close collaboration between designers and users.  Figure 6, describes\nthe commonly known stages of Design Thinking, namely, Identify and Define, Research\nand Plan, Produce, and Implement and lastly, Test and Evaluate [22].\nFigure 6:Design Thinking exemplar [59]\n3.1    Identify and Define\nThe first phase of the experiment involves the steps of Identification and Definition.  This\nstage is dedicated to carefully identifying and presenting the underlying problem. The pri-\nmary focus is on the systematic gathering of key information,  to attain a comprehensive\nunderstanding of the current issue.  To obtain this:  the study uses Structured Interviews as\nthe primary method for information gathering. Subsequently, these interviews will later be\ndecoded and used to construct a Persona, enriching our insights into the user experience\n[62] [22].\n3.1.1    Interviews\nThe first method used to gather information and understanding of the problem is by\nusing interviews. The type of interview used is a Structured interview, meaning a systematic\napproach where the interviewee asks the same predetermined questions to all candidates in","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":19,"lines":{"from":1,"to":20}}}}],["doc-33",{"pageContent":"the  same  order.   This  method  is  considered  to  be  approximately  twice  as  effective  as  a\ntraditional interview where the interview follows an unstructured standard [3]. For the sake\nof this project,  structured interviews have been used,  but similar results can be obtained\nthrough other interview styles such as semi-structured or unstructured interviews [61].\n14","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":19,"lines":{"from":21,"to":25}}}}],["doc-34",{"pageContent":"The interview was conducted with 15 questions and was estimated to take 30 minutes\nto complete. The target group to participate in the interview was people of any gender with\na Quality Assurance background.  All interviews were held in-house in the PayPal office,\nand conducted between myself and the selected PayPal employees.  The structure of the\ninterview followed the following category.\n1.  Question with User Profile focus\n2.  Questions with Test Case focus\n3.  Questions with Artificial Intelligence focus\n4.  Questions about ethics and Artificial Intelligence\nThe whole list of questions is available in the project appendix A.\n3.1.2    Persona\nThe second method for identifying the problem is to also understand whom the project\naims to develop a system for. In order to identify the user needs, one part of the methodology\nis to create a persona.  A persona can be described as a specific model of a user, that aims\nto resemble a typical user profile. Typically this persona is used to represent a specific user\npattern, behavior, or other motives [4].\nAccording to the study, one important part of the creation of a persona is to define the\ngoal. Meaning that the goal of the user should be used to direct the design. A persona is also\nan effective way of mapping and understanding the motives behind the user’s actions [4].\nBy using this purpose throughout the whole process of Design Thinking we can obtain a","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":20,"lines":{"from":1,"to":20}}}}],["doc-35",{"pageContent":"user-centered and goal-directed design. Figure 7 illustrates the used template when creating\na persona.\nFigure 7:Persona Template\n3.2    Research and plan\nThe second stage of the Design Thinking process is to research and start conducting a brain-\nstorming session for possible solutions to the identified problem. This can be created from\n15","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":20,"lines":{"from":21,"to":27}}}}],["doc-36",{"pageContent":"different methods such asMind mappingorRapid ideation[33].In the process of this study,\nthe Research and plan phase was heavily conducted based on a literature review. The review\nhas focused on the current exploration stages of automatically generating test cases.\n3.2.1    Literature study\nThe literary study was conducted by reviewing an extensive amount of scientific arti-\ncles and studies, which all have a base in Artificial Intelligence in Software Testing.  The\nliterature used for this part of the study was obtained through websites like IEEE or similar.\nIn order to get relevant articles to base the literary review on, keywords such as Automatic\nSoftware Testing, AI for Software Testing, or AI Test Cases were used.\n3.2.2    Exploring the company resources\nThe second methodology used in the Research and plan phase was to explore the company-\nwide resources.  If this study were reproduced, this method is not necessary to the same\nextent, and other solutions can be found.  Naturally, the exploration of company resources\nwill vary depending on the company, but overall the following key points were taken into\nconsideration when exploring the solution from a company point of view:\n•  What already existing tools can be found on the company cloud?\n•  What restrictions exist for using the specific tools?\n•  What access requests are needed?\n•  What is the legal point of view?\n3.3    Produce and Implement","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":21,"lines":{"from":1,"to":20}}}}],["doc-37",{"pageContent":"3.3    Produce and Implement\nThe third phase in the design thinking process is called Produce and Implement. This stage\ncan also be called the prototyping stage in other similar models to the design thinking pro-\ncess. From a technical point of view, the phase focuses on prototyping a possible solution,\nbased on the information of the previous stages.  In this context, the previous stages of the\nprocess gave sufficient information to set the direction of the project. The way of automati-\ncally generating test cases was chosen to be through Generative AI, and the specific model\nto use was chosen according to the company standards. In order to prototype and implement\na possible solution. The following model was used:\nModel nameLlama-2-7b-chat-hf\nModel size7B\nVersionVersion 1\nFrameworkOpenAI\nTable 2:Model information for the Llama2 model\nThe model described is Llama2, which is provided by Meta [36].  This model is used\nas the pre-trained model to communicate with. The model is hosted in-house and accessed\nthrough a private endpoint, but the study could in practice be implemented by saving the\nmodel locally.  To further implement the system to not only understand the knowledge of\n16","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":21,"lines":{"from":20,"to":38}}}}],["doc-38",{"pageContent":"the outside world (pre-trained data) but also understand the specific organization. The next\nsteps of the implementation include setting up the knowledge base that is used to set up the\nRetrieval Augmented Generation.\n3.3.1    Create Knowledge base\nTo create a knowledge base with the needed information from the company, the knowl-\nedge base is conducted through two main steps. Firstly the needed information is gathered,\nand then secondly the information needs to be stored.\nGather the data\nThe first step to creating the knowledge base is to gather the required data to store.  For\nthis specific implementation, the only supported file type is Text file (.txt). The files can in\npractice contain all forms of information that you want the answers to the LLM to take into\nconsideration.  This can for example be specific User Journeys or other useful information\nto solve your specific task.\nIn this specific use case, the end goal of the interaction with the LLM is to create Test\nCases.  Therefore the information that is relevant to store is documents explaining the spe-\ncific company domain, which in this study is limited to Onboarding Experience (OBEX).\nFor  example,  these  documents  can  contain  user  journeys,  screens,  components,  etc.   In\nan optimal solution the data should also container previously created Test Cases for other\nprojects in the same domain. Including project: Description, Requirements, and Acceptance\ncriteria.","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":22,"lines":{"from":1,"to":20}}}}],["doc-39",{"pageContent":"criteria.\nAll the desired documents are stored in one local folder, and loaded to the system by\nusing the classSimpleDirectoryReaderwhich is provided by llama\nindex [32]. The doc-\numents are loaded according to the following:\nreader = SimpleDirectoryReader(input_dir=\"../Name_of_folder/\")\ndocs = reader.load_data()\nThis information can be gathered through numerous ways, such as manually extracting\ntext from the company management tool or through API. In this implementation, the texts\nare imported manually to the local folder.\nSave as Embedding\nAfter the desired data has been gathered, each of the documents is loaded into the system,\nbefore each documents is slit into smaller pieces [50]. This is a process called Chunking and\nis a commonly used process in Natural Language Processing. There exist different methods\nwith the goal of chucking but for this implementation, a simple approach is used.\nThe approach for chucking the documents is to follow a similar approach as with the\nmethodCharacterTextSplitterfromLangchain. However, this method is not compat-\nible with the kernel where the in-house LLM is implemented, meaning that the Langchain\npackage cannot be used. A manual way of chunking is instead applied with similar charac-\nter asCharacterTextSplitter, which splits text based on characters (by default ”\\n\\n”),\nhowever, this is not recommended if the study would be replicated.\nThe next step of the process is to embed the chucks.  When embedding the different","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":22,"lines":{"from":20,"to":41}}}}],["doc-40",{"pageContent":"chucks, this is made by using an embedding model.  There are numerous versions of em-\n17","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":22,"lines":{"from":42,"to":43}}}}],["doc-41",{"pageContent":"bedding models that could be used, but similarly to the LLM model, the in-house embedding\nmodel is used, namelyllama-2-7b-chat-hf.  The following code section is an example\nprovided on the OpenAI website, on how to obtain embeddings:\nfrom openai import OpenAI\nclient = OpenAI()\ndef get_embedding(text, model=\"text-embedding -ada -002\"):\ntext = text.replace(\"\\n\", \" \")\nreturn client.embeddings.create(input = [text], model=model).data[0].\nembedding\ndf[’ada_embedding ’] = df.combined.apply(lambda x: get_embedding(x, model\n=’text-embedding -ada-002’))\ndf.to_csv(’output/embedded_1k_reviews.csv’, index=False)\nThe way to produce this can vary depending on the organization’s standard and pro-\ncedure but the results of the embedding have during this implementation been decided to\nbe stored locally.  Similarly, as the example provided above, this implementation saves the\nembeddings locally in a CSV file.  Alternative solutions to this, are to save the embedded\ndocuments in an embedding database such as Chroma [8], or in a cloud solution such as\nAzure OpenAI [37].\n3.3.2    Overview of the system\nFigure 8 illustrates an overview of the system as a whole.   The figure shows differ-\nent documents being embedded and saved in a vector space, which in this implementation\nmeans the locally stored CSV file.  From a user perspective, the user writes an input query\nthat similar to the documents are embedded with the same embedding model. The embed-","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":23,"lines":{"from":1,"to":23}}}}],["doc-42",{"pageContent":"ded query and the embedded document chunks are compared to each other and theKbest\ndocument chucks are selected. As described in section 2.2.2, the document chucks with the\nhighest degree of similarity, meaning the embedding points with the lowest distance from\nthe input query embedding point. The chunks with the highest degree of similarity are then\nsent to the LLM together with the input query as context, to combine the information from\nthe knowledge base with the pre-trained data, and from here produce an output.\n18","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":23,"lines":{"from":24,"to":30}}}}],["doc-43",{"pageContent":"Figure 8:An overview of the system.  It’s important to note that this implementation is\nspecifically designed for txt files, even though the image depicts a similar process\nfor PDFs\n3.3.3    Prompt Template and Llama parameters\nThe last part of the methodology in the Produce and Implement phase, has a focus point\nin finding a good template for prompt engineering. From the previous stages in the Design\nThinking process, the end-user of the system has been identified. In the process of this, we\nhave also identified some potential noise when using the system.  To minimize this noise,\nand to guarantee that the results given to the end-user are similar in quality, a suited Prompt\nTemplate needs to be identified.\nTo find the most appropriate template for the context, the information provided in the\nBackground 2.2.1, is used to create a well-suited prompt. In order to test this template, and\nto guarantee that the results were matching the expectation.  A playground of Llama2 is\nused [31]. This playground makes it available for the user to tune the following parameters:\n•  Model sizeLarger size means smarter but slower.\n•  System promptThis is used to help guide system behavior\n19","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":24,"lines":{"from":1,"to":17}}}}],["doc-44",{"pageContent":"•  TemperatureAdjusts randomness of outputs\n•  Max TokensMaximum number of tokens to generate\n•  Top PSamples from the top p percentage of most likely tokens\nThe playground is used to adjust parameters and find the most suited for the context. In\norder to test this the following prompt template was used:\nI am a Quality Assurance Engineer and need you to act as an expert in software\ntesting. Generate all test cases for project DTPAAABBBDG-1241. The test cases\nshould follow a table format with columns: number, scenario, test steps, and expected\noutcome\nTo  facilitate  the  implementation,  the  knowledge  base  includes  a  document  with  the\nproject id DTPAAABBBDG-1241,  where in the same document the project description,\nrequirements,  and  acceptance  criteria  are  located.   The  same  results  can  be  obtained  by\nstating these directly in the prompt.\n3.4    Test and Evaluate\nThe fourth and final phase in the design thinking process is called Test and Evaluate.  This\nphase focuses on exhibiting and evaluating the created system.  To do this, the project is\nrefined and a specification on how to practically evaluate the system is created.\nFor this project, the evaluation of the system will be conducted through both quantitative\nand qualitative results.  The way of evaluating the system is by practically answering the\nfollowing questions:\n1.  Is the output of the system responding to the accurate task. Meaning the system gives","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":25,"lines":{"from":1,"to":21}}}}],["doc-45",{"pageContent":"Test Cases with the correct format, columns, names, etc.\n2.  How many of the test cases are accurate to the specific project?  This means giving\nthe defined inputs, how many of the Test Cases are useful (%). This result is obtained\nby comparing the output from the created system and comparing this to test cases\ncreated manually by a QA.\n3.  How many of the Test Cases are considered noise?  Meaning non-accurate data, hal-\nlucinations, etc (%). This result is obtained by comparing the output from the created\nsystem and comparing this to test cases created manually by a QA.\n4.  Specific results representing Happy paths respective Edge Cases.\nDefining expectation of the system\nGiving the following input the system is expected to give the following output:\n•  Input:  A prompt including project-id linked to the project Description, Acceptance\ncriteria, and Requirements as a string.\n20","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":25,"lines":{"from":22,"to":35}}}}],["doc-46",{"pageContent":"•  Output: A table with columns; Test case number, Scenario, Test steps, and Expected\noutcome. All test cases involved in each scenario should be in the same cell.\nUseful terminology to interpret the results [52]:\n•  Test Case:  Tasks needed to validate a particular feature or functionality during soft-\nware testing.\n•  Happy path: Specific type of test case, conducting testing solely based on the accep-\ntance criteria for the feature.\n•  Edge Case:  Specific type of test case, exploring scenarios beyond the foundational\nassumptions, discovering alternative ways to utilize a feature that may not have been\ninitially intended.\n•  Noise:  Unwanted or random interference.  This can for example be hallucinations,\nincorrect outputs, or similar.\n21","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":26,"lines":{"from":1,"to":13}}}}],["doc-47",{"pageContent":"4   Results\nThe following chapter contains the main results of the study. In order to present the results\nin an effective way, the results are divided into parts representing the steps of the Design\nThinking process. The first part of the result section displays the findings from the interview,\nwhich is taken into consideration to create the Persona for the project. The interview section\nbelow provides an overall understanding of the results, which is decoded from the interview\nanswers.\n4.1    Interviews\nThe interview conducted was answered by 5 interviewees, all of the whom are a part of\nthe Quality Assurance chapter.  The 15 questions asked during the interview have given a\nvariation of answers that are sufficient to gather an end-user understanding. In the following\nparagraph, the interview results will be summarized according to section 3.1.1.\nFrom the first section of the interview, the goal was to get to know the overall user being\ninterviewed. The results from the first section indicate that all the people being interviewed\nhave a Quality Assurance background, but the length and involvement in the current com-\npany can vary in a range of 2-5 years.  One of the interviewees also has a background in\nAcademic Research which is also important to keep in mind when analysing the results.\nThe second section of the interview had the goal to gather an overall understanding on\nthe Test Case generation and experience of the interviewees. The entire process of creating","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":27,"lines":{"from":1,"to":19}}}}],["doc-48",{"pageContent":"test cases is documented and the journey and collaboration with other roles in the com-\npany is carefully being considered. Thought this section, we could gather understanding of\nwhat the relevant documents are when creating test cases in the current stage.  As the in-\nterview reveals, currently the interviewees use documents such the project requirement and\nacceptance criteria. Overall the interviewees tend to collaborate together with the develop-\ners, designers and project managers in order to gather an overall understanding on what is\nexpected in the scope of the project.\nThe interview results also indicated a variety of answers in the question regarding fre-\nquency  of  creating  test  cases.   Most  of  the  interviews  suggested  that  the  amount  of  test\ncases is highly related to the type of feature.  If the feature is new or considered more ex-\nperimental.  Then the test cases tend to be created and updated more frequently.  From the\nanswers we can also conclude that the process related to creating test cases isn’t explained\nas annoying but specifically the wordrepetitiveis used for the majority of interviewees.\nThis could be explained because of the Test Cases being created for the specific QA to map\nthe feature and possible user journeys.  But since the document is rarely reviewed by other\nproject roles, the act of updating the document, falls behind and becomes less prioritised","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":27,"lines":{"from":20,"to":35}}}}],["doc-49",{"pageContent":"than other responsibilities related to the QA role.  This leads to the documents sometimes\nbeing outdated.\n22","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":27,"lines":{"from":36,"to":38}}}}],["doc-50",{"pageContent":"The third part of the interview focuses on Artificial Intelligence,  both with question\nrelated to experience and more practical exercises.  All of the interviewees had experience\nin using Large Language Models, but all of them had only used ChatGTP and not another\nLLM. The majority of the interviewees have not used such LLM tools to solve work related\ntasks, but mostly uses the tools to solve linguistic tasks such as rephrasing or summarizing.\nOne of the interviewees has stated that the tool also is commonly used for that person to get\ninspiration or to used as a starting point in creative sessions.  Non of the interviewees had\nany clear knowledge of the company specific LLMs nor where these can be found.\nFrom the more practical exercises. The last questions of the section involves a practical\nexercises that reveals important capabilities related to prompt engineering.  The simplified\ninstruction that was given to the user to was the following questions. The goal of the exercise\nwas to formulate a prompt to retrieve as precise and qualitative answers as possible.\n•  Q1: 3 recipes for a blog.\n•  Q2: An analysis of the book A Little Life.\n•  Q3: Test case for a simple login function at Facebook.\nThe outputs provided by ChatGTP was positively received by all of the interviewees.\nThe interviewed people expressed a positive response both from the matter of quality of the\ntest case provided, but also about the coverage. The results however clearly indicate a lack","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":28,"lines":{"from":1,"to":18}}}}],["doc-51",{"pageContent":"of knowledge of Prompt Engineering.  From the suggested theory 2.2.1 the results show a\nclear indication of prompt engineering not being applied in a sufficient matter.  Figure 9\nillustrates how many of the interviewees actually use the components previously discussed\nin section 2.2.1.   As the figure illustrates,  all five interviewees use the component Task,\nwhile some interviewees occasionally use Context. But a very limited representation of the\nother components is found.\nFigure 9:Component representation during interview prompting exercise\nThe last part of the interview contextualise the fears and worries of involving Artificial\nIntelligence in the process of Software testing.  From an ethical point of view, considering\nGenerative AI possibly being applied to create Test Cases, the interviewees expressed little\n23","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":28,"lines":{"from":19,"to":29}}}}],["doc-52",{"pageContent":"fear for the specific task.  However multiple interviewees expressed a worry regarding the\npossibility for Code and Company leakage.  This is however not directly applicable in this\ncase but rather a general worry when involving AI around more sensitive information.  All\nof the interviewees however, did express that Test Case generation usually does not involve\na very high degree of sensitive data, or has a business critical function in the product, which\ncontributes to the fear and worry being decreased.\n4.1.1    Persona\nFrom the interview answers a persona was able to be created. This persona is intended\nto act as a reflection of the intended end user.\nFigure 10:Created Persona\n4.2    Evaluate the system\nBy evaluating the system according to section 3.4, the following results where obtained. For\nthe first evaluation point:Is the output of the system responding the accurate task. Meaning\nis the system giving Test Cases with correct format, column names relevant to the specific\nproject.  The results show that the implementation correctly uses the project-id to gather\ninformation and overall produces test cases in the correct format. Specifically meaning that\nthe format given by the system is in a table format with the corrected columns and overall\ncomposition. However, the column ”Test Steps”, seems to at times, be getting inconsistent\nresult. All steps in a specific scenario are not always formatted in a numerical list, which is","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":29,"lines":{"from":1,"to":19}}}}],["doc-53",{"pageContent":"likely to be due to the limited data of previous test cases.\nThe following Figure 11 represent a data comparison between the AI system output and\nthe test cases created manually of one specific project.  The bar plot shows a side-by-side\ncomparison  between  these,  with  y-axis  representing  the  amount  and  x-axis  representing\ncategories being Happy Paths, Test Cases, Edge Cases and Hallucinations.  As seen in the\nfigure, the category Hallucination only appear for the AI-system bar since this is only appli-\ncable here.  The figure also reveals a result of 0.5 in hallucinations on that specific project,\nwhich symbolises that the test case is not in particular fully noise but rather contains parts\nof noise.\n24","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":29,"lines":{"from":20,"to":29}}}}],["doc-54",{"pageContent":"Figure 11:Data comparison between AI system and manually created test cases\nThe following Table 3, reveals some specific comparisons between the AI system and\nManual way of writing test cases.  As seen in the table,  the time decreased significantly\nby using AI-driven test case generation, however it is important to keep in mind that only\n66.67% of the test cases are practically useful but could contain minor noise.  The project\nused to test this, is the same project revealing the numbers from Figure 11 and is a relatively\nspecific and small project.  The table also reveals the similarity score discussed in Section\n2.2.2.  As seen in the table the K, in this case five, most similar chucks selected to be used\nto solve the query have a similarity score between 0.669 and 0.511, where a score between\n0 and 1 is possible. A higher similarity score indicates a higher probability that the selected\ndocument will contain relevant information to the input query.\nThe following Figure 12, deep dives into the reason why a specific test case have been\ncategorised  as  Noise  in  Figure  11.   As  previously  discussed,  noisy  data  or  meaningless\ndata can be due to many reasons.  Some of the data is categorised as a Hallucination can\nbe described as ”generated content that is nonsensical or unfaithful to the provided source\ncontent” [63]. For example this is when the system starts producing something entirely un-","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":30,"lines":{"from":1,"to":16}}}}],["doc-55",{"pageContent":"related to the prompt.  Another reason why something can be categorised as noisy is if the\nproduced outcome is non accurate.  Meaning it is not complete nonsensical, as for a hallu-\ncination, but still isn’t accurate to the project.  For example this could be including screens\nor components that don’t exists etc.  Another reason why test cases can be categorised as\nnoise is due to Non accurate tech definition.  Which in this project is defined as a tech def-\n25","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":30,"lines":{"from":17,"to":22}}}}],["doc-56",{"pageContent":"Table 3:Time and usefulness summary\ntitle\nTime per use caseAI  system:    23.75  seconds  per  Use\nCase,  Manual:   600  seconds  per  use\ncase [40]\nOverall usefulness of AI systemUseful test cases: 66.67%\nHighest  similarity  between  query  and\ndocuments (K=5)\n•  relatedness= 0.669\n•  relatedness= 0.656\n•  relatedness= 0.615\n•  relatedness= 0.556\n•  relatedness= 0.511\nQA found usefulYes 4/5\ninition that is not understood correctly. For example a tech acronym SPI is confused in the\nsystem to be Serial peripheral interface (SPI) instead of the meaning in the organisation,\nwhich is Service Provider Interface (SPI). Because of confusion in tech acronyms this can\ncause inconsistency in the system and can make it harder for the system to draw an accurate\nconclusion.  Since the results presented in Figure 11 are reflecting the results of a specific\nproject, the pie chart 12 instead starts exploring outside the specific project.  This pie chart\nrepresent the most commonly appearing noise the system tend to produce when retrieving\ninformation in order to produce queries.  In the process of exploring the commonly pro-\nduced noise,  the Figure 12 reveals that most commonly,  Non accurate data is produced,\nwith a result of 62.5%.  This implies that the generated data is not entirely nonsensical; in-\nstead, it may include information that falls within the project’s scope but is not necessarily\napplicable. For instance, the system might generate a step in a test case commonly found in","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":31,"lines":{"from":1,"to":26}}}}],["doc-57",{"pageContent":"similar projects, even if it is not implemented in the specific organization at hand.\nFigure 12:Reason for Noise label\n26","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":31,"lines":{"from":27,"to":29}}}}],["doc-58",{"pageContent":"Table 4 illustrates an output instance generated by the system.  The output provided by\nthe system, displays a Happy Path following a User Journey of a specific costumer type. The\nsystem output initiates with the message, ”Great, I’d be delighted to assist you in creating\ntest cases for theproject-idproject. In accordance with the provided articles, here are some\ntest cases for your consideration:” followed by the presentation of table 4.\nTable 4:Example of system output given the defined input prompt from section 3. In order\nto make this table more readable the columns and rows are inverted in comparison\nto the actual system output. This table also just shows one specific test case, where\nthe actual output is a collection of all test cases of a project.\nGenerated output\nTest case1\nScenarioSuccessful signup flow for an existing PayPal\nmerchant\nPreconditionsThe  user  has  an  existing  PayPal  account  +\nThe user has not previously signed up for the\nZettle product.\nTest steps\n1.  Navigate  to  the  signup  URL  (Specific\nsign up flow in stage)\n2.  Enter the required information for the\nsignup  flow,  including  the  PayPal  ac-\ncount  email  address,   password  (cor-\nrect),   and  business  type  (INDIVID-\nUAL).\n3.  Fill  out  the  form  with  valid  informa-\ntion  (e.g.monthly  sales  of  $1000,\nSSN  number  123-45-6789,  work  mo-\nbile number 555-123-4567)\n4.  Verify  that  the  signup  process  com-\npletes successfully and the user is redi-\nrected to the Zettle dashboard.","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":32,"lines":{"from":1,"to":31}}}}],["doc-59",{"pageContent":"rected to the Zettle dashboard.\nExpected outcomeThe user should be able to successfully sign\nup for the Zettle product using their existing\nPayPal account.\n27","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":32,"lines":{"from":31,"to":35}}}}],["doc-60",{"pageContent":"5   Discussion\nIn Section 4, the given results are presented.  As this study aims to investigate how the in-\ntegration of AI-driven test case generation in software development pipelines impacts the\noverall software quality and development efficiency, we can from these results draw some\nconcrete conclusions.   The results show that the AI-driven test cases,  generated through\nLLMs can generate output test cases in the correct format relevant for a given input.  The\nsystem draws some conclusions resulting in the test case structure and uses the informa-\ntion from the knowledge base to create organisation specific test cases.  During the Design\nThinking first phase,Identify and Define, we can also conclude that the prompt engineering\nknowledge appears to be limited among the employees involved in the study.  This finding\nwas relevant to create a more coherent end-user friendly system for the intended end user\npresented in Figure 10.  The quality of the test cases is shown in Figure 3, where the study\ncompares the results of the AI-driven test cases against the Manually written test cases. To\nnot only compare quality but also time, in order to draw sufficient conclusions towards the\nobjective, Table 3, also displays results in the form of time, similarity score, and feeling of\nthe system.\nBy specifically looking into certain areas of Section 4, we can make more discussions\nrelevant to the study. As previously described, during the first phase of the design thinking","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":33,"lines":{"from":1,"to":18}}}}],["doc-61",{"pageContent":"process, Identify and Define, we can observe a clear indication concluding that the knowl-\nedge of Prompt Engineering is limited.  Even though direct knowledge isn’t tested during\nthe interview, the interviewees show signs pointing toward that the practical application of\nprompt engineering is limited. In Figure 9 the usage of prompt engineering components is\ndisplayed in correlation to the practical exercises in the interview. As seen in the figure, only\ntheTaskcomponent is consistently used by the interviewees, whileContextis occasionally\nused.  All the other components are not used in any degree which according to the Theory\n2.2.1, is not recommended.\nBased on the prompt style and in combination with the analysed quality of response\nfrom the LLM during the interview, we can conclude that the limited knowledge of prompt\nengineering could lead to inconsistent responses to the QAs.  To limit this, the decision to\nuse a given prompt template was made. This was created to provide the user an easy access\nto a proper way of prompting, which would potentially lower the risk of the answer quality\nbeing  affected  by  the  knowledge.   Since  the  system  is  intended  to  be  used  by  a  typical\nuser,  as presented in Figure 10,  the system outputs must provide reliable and consistent\nanswers. Why the application of prompt engineering components is as limited as the results\nshow in Figure 9 could be because of a lack of knowledge but also from the misconception","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":33,"lines":{"from":19,"to":35}}}}],["doc-62",{"pageContent":"of using a chatbot.  Just as customers commonly formulate messages forhelp chat-bots,\nusers might anticipate a similar level of ease when interacting with the LLM. While, from\na practical standpoint,  this interaction bears similarities,  it’s crucial to note that,  for this\nparticular system, the task demands a greater level of data and comprehension.  This also\nleads to the user needing to make some more specific customization techniques such as\nPrompt Engineering. From a company point of view, the lack of knowledge in the domain of\n28","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":33,"lines":{"from":36,"to":42}}}}],["doc-63",{"pageContent":"Prompt Engineering could be avoided by teaching and spreading knowledge and awareness\nof the subject on a company level.\nIn Figure 11 in combination with Table 3, we can observe the direct relevant results for\nthe objective of the study.  Figure 11 shows the direct comparison between the AI-driven\ntest cases against the Manually written test cases.  As seen in the results we can see some\ncapabilities that the system can write Test Cases. For example, the figure displays to which\ndegree the system, with its current implementation, can identify the Happy Path in the test\ncases and some degree of edge cases.  The representation of Edge Cases in these results\nshows a lower representation in the AI-driven test cases. Why this is appearing can be dis-\ncussed to be because of the limitation in data. The implementation is created corresponding\nto the theory described in section 2.2.2.  The system uses an external knowledge base in\norder to make conclusions not only depending on the pre-trained data but also take into\nconsideration new data.  The limitations of data in the knowledge base could be a reason\nwhy these specific results are appearing. One observed issue is that certain documents in the\nknowledge base may be somewhat outdated. Other problems might include that the lack of\ndata in the knowledge base forces the LLM to make conclusions without necessarily having\nall the needed information, which could lead to assumptions being made and noise being\nproduced.","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":34,"lines":{"from":1,"to":18}}}}],["doc-64",{"pageContent":"produced.\nOn the other hand, one interesting observation is the Similarity Score provided in Table\n3.  The five best chunks to the given input gave similarity scores between 0.511 and 0.669,\nhowever even if the score does not necessarily appear high, the quality of the output is still\nfairly positive. This result is interesting due to the methodology presented in section 3. The\nparameters used when interacting with the model can be specified, taking into consideration\nthis study. By these results, we can conclude that similarity scores don’t necessarily need to\nbe at a very high degree to produce interesting results.\nIf we look into the results in Table 3, the time-efficiency increase is clear in comparison\nto the manually written test cases.  Which directly affects theCore NeedsandFrustrations\nwhich  are  identified  and  summarised  in  the  Persona  10.   One  other  frustration  that  was\nidentified during the interviews was the difficulties that QAs face when having to which\nbetween domains.  Since this specific study is mainly investigating the generation of test\ncases, this switch between domains is not part of the scope. But with the technology used in\nthe implementation, meaning Retrieval Augmented Generation, described in section 2.2.2.\nWith more data and also data from other domains, this could facilitate for QAs to easily find\ninformation and therefore also facilitate switches between domains.  Since the user is not","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":34,"lines":{"from":18,"to":34}}}}],["doc-65",{"pageContent":"bound to only prompt questions resulting in test case generation, the user could in practice\nask anything including summarizing information from other domains.\nIn the same Table 3, we can also observe some general attitudes towards the usage of\nAI for generating test cases.  The participants had a positive view of using AI, which also\ncorresponds with the initial thought from the first interview. In a general matter, the common\nreflection is that this tool could be used for facilitating and increasing work efficiency. Even\nif the implementation is not itself sufficient at the current state to fully use the test cases.\n66.67% of the test cases are considered useful. Important to notice is also the attitudes and\nspecifically fears with using AI. As seen in the interview section of the results chapter 4,\nthe interviewees have also expressed a certain degree of fear towards company information\nleakage.  Since this specific implementation locally stores documents and embedding, the\npossibility of leakage is fairly slim. However, if this study would continue as a future work.\nThe way of storing data is optimally going to change. This case would require a safer way\n29","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":34,"lines":{"from":35,"to":48}}}}],["doc-66",{"pageContent":"of storing a larger amount of data, to minimize the risk of company information leakage.\nLastly, Table 4 displays an example of a system output. The table illustrates one specific\ntest case in else a collection of test cases that the system produces.  The example output is\na Happy Path, which in general is the type of test case the AI tends to find more easily, as\nseen in 11. The table reveals how the AI correctly identifies both the task and the necessary\ninformation for the task. As seen in the initial message before the table, the AI finds articles\nrelevant to the query. The language used is straightforward and could be considered casual,\nwhich likely is due to the componentTonediscussed in section 2.2.1 was never used in the\nPrompt Template used in the Method 3. However, the results were not always displayed with\nthe same quality.  As described in the results 4, the project information used for the study,\nwas a of simpler sort. Increasing the complexity, or asking about test cases for projects that\nrequire significantly higher knowledge might likely change these results.\nBy comparing the results already discussed to my initial expectations, some of these re-\nsults are unexpected. Initially, as I did not have sufficient knowledge about any Customiza-\ntion techniques, discussed in Section 2.2, I did not know about the critical role Prompt Engi-\nneering would have in this project. Initially, my thought was that a slightly different prompt","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":35,"lines":{"from":1,"to":16}}}}],["doc-67",{"pageContent":"would drastically change the quality of the output.   But from reading and understanding\nwhat Transformer Architecture 2.1.2 is, and how the system is connected, the results started\nto be more understandable.  As described in Section 2.2.2, the input query are embedded\nin the same way as the documents composing the knowledge base.  The documents used\nto answer the input query is selected depending on the closest distance between the input\nquery embedding and the document embedding.  Having this in mind, the importance of\nprompting the input query sufficiently is much more understandable.\nAnother unexpected finding was the results obtained and how these results were in some\nway dependent on the chunk size and model size.  The scaling Laws described in section\n2.1.3  describe  the  practical  explanation  of  why  LLM  tends  to  perform  better  than  NLP.\nFrom  the  implementation  using  Retrieval  Augmented  Generation,  I  initially  thought  the\neffect would be more positive.  However, the system seemed to be confused with the lack\nof data composing the knowledge base, which led to assumptions being made. In the same\nsection 2.1.3, it is also discussed how this scaling law works to predict on task level.  The\ndiscussion touches on how no direct correlation to heightened performance can necessarily\nbe made on a task level. This would be an interesting comment for this study since AI-driven\ntest case generation is highly task-driven.\n5.1    Difficulties and solutions","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":35,"lines":{"from":17,"to":34}}}}],["doc-68",{"pageContent":"5.1    Difficulties and solutions\nDuring the time of the study, some difficulties were faced. When working with a language\nmodel in a big organisation, multiple limitations are set.  For example, the specific Kernel\nused was not compatible with the recommended packages for the solution. Which leads to\nchanges being forced to be made that could potentially have a negative effect on the results.\nOne of the packages being affected by this incompatibility issue isCharacterSplitter.\nBecause the package was not compatible with the current kernel, the package was excluded\nand a type of manual slit was made.  The manual split however is very simplified and does\nnot exactly behave as the package.\nAnother problem faced was the token-size sent into the LLM was too big when collect-\ning the 5 best points for the Retrieval Augmented Generation. At the start of the project, it\n30","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":35,"lines":{"from":34,"to":45}}}}],["doc-69",{"pageContent":"was noted that fewer points would be considered, but because of the composition of the doc-\numents, this was changed.  By using more points, meaning taking into consideration more\ntext, this often leads to the token size exceeding its limit, forcing the API to fail.  To solve\nthis, instead of directly sending in the documents together with the input query, as discussed\nin section 3, the documents selected are summarized first, to avoid the vast token size. This\nway of solving the problem showed pleasant results, but could in the further continue to be\nworked on to create even better summaries. By taking into consideration the section regard-\ning transformer architecture 2.1.2. Since the componentOutputs (shifted right), predicts the\nnext word in a sequence by looking at the words before it.  The summarisation of the text\nplays a very critical role, considering this is the text that will be sent to the LLM.\n31","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":36,"lines":{"from":1,"to":11}}}}],["doc-70",{"pageContent":"6   Conclusion\nIn this study,  the main objective was to investigate how the integration of AI-driven test\ncase generation in software development pipelines impact the overall software quality and\ndevelopment efficiency. In particular the study uses Large Language model and Embedding\nmodel provided by Llama, specifically Llama2 of size 7B, in order to generate test cases\ngiven a defined input. The implementation uses the customization technique Retrieval Aug-\nmented Generation (RAG) in order to locally story organisation information and from this\nstored information make conclusions resulting in test cases.\nThe study indicates that by using the customization techniques Retrieval Augmented\nGeneration, in combination with Prompt engineering, the implementation is able to generate\ntest cases in the correct format and to some degree useful.   One of the results obtained,\n66.67% of the test cases for a particular project where considered useful.  However, since\nthe project was of more simple character these results might be different depending on the\nproject complexity.  As seen in the results 4, the most common noise in the system is due\nto the system producing non accurate data.  From the results we can also conclude that by\nusing the AI system the time efficiency increased significantly.  Even if all results where\nnot useful, and some useful results created minor noise.  The overall time efficiency is still\nprovides a very pleasant impact on the development efficiency.","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":37,"lines":{"from":1,"to":18}}}}],["doc-71",{"pageContent":"Despite the finding of the investigation, the study is not sufficient enough to draw strong\nconclusions.  As the study is anearly phase exploratory study, these results might be help-\nful for the future investigations on the subject.   Large Language Models and the chosen\ncustomization techniques have shown great potential in customizing LLMs to understand,\ninterpret and draw conclusions on company information. The implementation show positive\neffect on development efficiency and could be positive to overall software quality.  Even if\nthe implementation do not show a direct positive effect on overall software quality, the time\nefficiency could be argued to help the overall software quality since the implementation can\nbe used for test cases of more simple character.  Helping the QAs to focus on finding edge\ncases and overall gain a better software quality.\n6.1    Future work\nEven if the results presented in section 4, show a positive indication for using AI-driven test\ncases, in terms of work efficiency. This study is still very limited, both in performance and\nin terms of coverage.  In the future, the implementation should ideally be based on much\nmore data.  This to cover all areas of the desired domain and to minimize the amount of\nconfusion caused by assumptions.  One type of data that was not used but should ideally\nbe  interesting  to  include  is  Manually  written  Test  Cases  from  other  projects.   This  was","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":37,"lines":{"from":19,"to":35}}}}],["doc-72",{"pageContent":"originally intended to be a part of the implementation but since previous test cases in this\nspecific domain were limited, only other domains test cases would be available to include,\nwhich was not suited to the implementation since this study is a prototype and only covers\n32","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":37,"lines":{"from":36,"to":39}}}}],["doc-73",{"pageContent":"one specific domain.  Introducing manually and well-written test cases could help the AI\nto solve the specific problem since it has previous data on how this has been made.   At\nthis time, the implementation makes conclusions and writes test cases with the information\nobtained, in combination with the information of test cases from the pre-trained data.  But\ngathering specific ways of writing test cases in the organisation would potentially be much\nbetter.  Lastly, as discussed in section 2.1.3, more data could lead to better performance,\neven if this is not guaranteed.\nAnother way of approaching the study in the future is to explore more layers from the\nsection 2.2.  In this specific implementation, the study has explored the usage of Prompt\nEngineering 2.2.1 and Retrieval Augmented Generation 2.2.2.  But in the future, it would\nbe interesting to see how the implementation would perform by using Fine Tuning instead.\nFine tuning in comparison to Retrieval Augmented Generation does not introduce a new\nset  of  data  but  rather  uses  a  dataset  of  labeled  examples  to  update  the  weights  of  LLM\nand make the model improve its ability for specific tasks [47].  While the usage of Fine\nTuning requires an extensive amount of data, this method also tends to give much better\nresults [51].  The possibility of the system to improving by introducing this method could\nbe interesting in future work.  Another way to most likely improve the performance of the","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":38,"lines":{"from":1,"to":17}}}}],["doc-74",{"pageContent":"system in this study is by using a different LLM model.   As described in section 3,  the\nLarge Language Model and Embedding Model are both of size 7B. This means that the\nmodel was pre-trained on 2 trillion tokens of data from publicly available sources [14].  In\nthe documentation available on the Huggingface website, a table representing the different\nmodel’s evaluation results. Figure 13 shows the results on standard academic benchmark.\nFigure 13:Evaluation Results from Llama documentation\nAs previously discussed, this uses the 7B size of Llama, which is the poorest performing\nModel version according to the evaluation results in Figure 13. Therefore exploring Llama\n33","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":38,"lines":{"from":18,"to":26}}}}],["doc-75",{"pageContent":"models of size 13B and 70B, would also potentially be an interesting future work of the\nstudy.  Especially in relation to the scaling law described in section 2.1.3.  Specifically, it\nwould be interesting  to investigate how much  better the system could  perform by using\nbigger models but still performing a specific task, being to create Test Cases.\nAnother interesting future work topic, similar to changing the LLM size, is to change the\nway the embedding is being made. As discussed in the text both Contextual embedding and\nWord embedding, among others, are ways we can in practice embed our documents 2.1.1.\nIn this implementation the embedding modelllama-2-7b-chat-hf,  however,  there are\nother specific algorithms for contextual respective text embedding that could be interesting\nto investigate to increase the performance of the system.\n34","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":39,"lines":{"from":1,"to":11}}}}],["doc-76",{"pageContent":"References\n[1]  Airfocus.   What  are  Quality  Assurance  Engineers  (QA  Engineers)?   Definition  &\nFAQ — airfocus — airfocus.com.https://airfocus.com/glossary/what-are-\nquality-assurance-engineers/. [Accessed 04-01-2024].\n[2]  Amazon.    What  are  Large  Language  Models?    -  LLM  AI  Explained  -  AWS  —\naws.amazon.com.https://aws.amazon.com/what-is/large-language-model/\n#:\n ̃\n:text=Large%20language%20models%20(LLM)%20are,decoder%20with%\n20self%2Dattention%20capabilities. [Accessed 04-01-2024].\n[3]  Nikoletta    Bika.How    to    conduct    a    structured    interview.https:\n//resources.workable.com/tutorial/conduct-structured-interview#:\n ̃\n:text=A%20structured%20interview%20is%20a,likelihood%20of%20a%\n20bad%20hire. [Accessed 08-01-2024].\n[4]  Stefan Blomkvist. Persona–an overview.Retrieved November, 22:2004, 2002.\n[5]  Browserstack.    How  to  write  Test  Cases  (with  Format  &  Example)  —  Browser-\nStack  —  browserstack.com.https://www.browserstack.com/guide/how-to-\nwrite-test-cases#:\n ̃\n:text=What%20is%20a%20Test%20Case,necessary%\n20to%20verify%20a%20feature. [Accessed 08-01-2024].\n[6]  Yihan Cao,  Siyu Li,  Yixin Liu,  Zhiling Yan,  Yutong Dai,  Philip S Yu,  and Lichao\nSun.  A comprehensive survey of ai-generated content (aigc): A history of generative\nai from gan to chatgpt.arXiv preprint arXiv:2303.04226, 2023.\n[7]  Yupeng Chang,  Xu Wang,  Jindong Wang,  Yuan Wu,  Kaijie Zhu,  Hao Chen,  Linyi","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":40,"lines":{"from":1,"to":26}}}}],["doc-77",{"pageContent":"Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al.  A survey on evaluation of\nlarge language models.arXiv preprint arXiv:2307.03109, 2023.\n[8]  Chroma.Home—Chroma—docs.trychroma.com.https://\ndocs.trychroma.com/. [Accessed 08-01-2024].\n[9]  Coursera.Machine   Learning   vs.   AI:   Differences,Uses,and   Benefits.\nhttps://www.coursera.org/articles/machine-learning-vs-ai#:\n ̃\n:text=\nIn%20simplest%20terms%2C%20AI%20is,can%20perform%20such%20complex%\n20tasks. [Accessed 04-01-2024].\n[10]  Ian Goodfellow, Yoshua Bengio, and Aaron Courville.Deep Learning.  MIT Press,\n2016.http://www.deeplearningbook.org.\n[11]  Google. Duet AI for Developers — cloud.google.com.https://cloud.google.com/\nduet-ai. [Accessed 08-01-2024].\n35","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":40,"lines":{"from":27,"to":41}}}}],["doc-78",{"pageContent":"[12]  Harvard.From   Transformer   to   LLM:   Architecture,   Training   and   Usage   —\nscholar.harvard.edu.https://scholar.harvard.edu/binxuw/classes/machine-\nlearning-scratch/materials/transformers. [Accessed 08-01-2024].\n[13]  Christina  Hsiao.   From  Simple  to  Sophisticated:  4  Levels  of  LLM  Customization\nWith  Dataiku  —  blog.dataiku.com.https://blog.dataiku.com/4-levels-of-\nllm-customization-with-dataiku. [Accessed 04-01-2024].\n[14]  Hugginface.  meta-llama/Llama-2-7b · Hugging Face — huggingface.co.https://\nhuggingface.co/meta-llama/Llama-2-7b. [Accessed 07-01-2024].\n[15]  Purva Huilgol. Top 4 Sentence Embedding Techniques using Python — analyticsvid-\nhya.com.https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-\nembedding-techniques-using-python/. [Accessed 08-01-2024].\n[16]  IBM. What is Deep Learning? — IBM — ibm.com.https://www.ibm.com/topics/\ndeep-learning#:\n ̃\n:text=Deep%20learning%20is%20a%20subset,from%\n20large%20amounts%20of%20data.[Accessed 08-01-2024].\n[17]  Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu,\nYiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented genera-\ntion.arXiv preprint arXiv:2305.06983, 2023.\n[18]  Michael I Jordan and Tom M Mitchell.  Machine learning:  Trends, perspectives, and\nprospects.Science, 349(6245):255–260, 2015.\n[19]  Jared  Kaplan,  Sam  McCandlish,  Tom  Henighan,  Tom  B  Brown,  Benjamin  Chess,","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":41,"lines":{"from":1,"to":22}}}}],["doc-79",{"pageContent":"Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws\nfor neural language models.arXiv preprint arXiv:2001.08361, 2020.\n[20]  Mohd  Ehmer  Khan  et  al.   Different  approaches  to  white  box  testing  technique  for\nfinding errors.International Journal of Software Engineering and Its Applications,\n5(3):1–14, 2011.\n[21]  Mohd Ehmer Khan and Farmeena Khan. A comparative study of white box, black box\nand grey box testing techniques.International Journal of Advanced Computer Science\nand Applications, 3(6), 2012.\n[22]  Lucy Kimbell. Rethinking design thinking: Part i.Design and culture, 3(3):285–306,\n2011.\n[23]  Manish Kumar, Santosh Kumar Singh, RK Dwivedi, et al.   A comparative study of\nblack box testing and white box testing techniques.International Journal of Advance\nResearch in Computer Science and Management Studies, 3(10), 2015.\n[24]  Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger.  From word embed-\ndings to document distances. InInternational conference on machine learning, pages\n957–966. PMLR, 2015.\n[25]  Yann  LeCun,  Yoshua  Bengio,  and  Geoffrey  Hinton.Deep  learning.nature,\n521(7553):436–444, 2015.\n36","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":41,"lines":{"from":23,"to":41}}}}],["doc-80",{"pageContent":"[26]  Omer Levy and Yoav Goldberg. Dependency-based word embeddings. InProceedings\nof the 52nd Annual Meeting of the Association for Computational Linguistics (Volume\n2: Short Papers), pages 302–308, 2014.\n[27]  Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K\n ̈\nuttler, Mike Lewis, Wen-tau Yih, Tim Rockt\n ̈\naschel, et al.\nRetrieval-augmented generation for knowledge-intensive nlp tasks.Advances in Neu-\nral Information Processing Systems, 33:9459–9474, 2020.\n[28]  Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham\nNeubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in\nnatural language processing.ACM Computing Surveys, 55(9):1–35, 2023.\n[29]  Peter  J  Liu,  Mohammad  Saleh,  Etienne  Pot,  Ben  Goodrich,  Ryan  Sepassi,  Lukasz\nKaiser, and Noam Shazeer.   Generating wikipedia by summarizing long sequences.\narXiv preprint arXiv:1801.10198, 2018.\n[30]  Qi Liu, Matt J Kusner, and Phil Blunsom. A survey on contextual embeddings.arXiv\npreprint arXiv:2003.07278, 2020.\n[31]  Llama.  Chat with Llama 2 — llama2.ai.https://www.llama2.ai/.  [Accessed 08-\n01-2024].\n[32]  Llamaindex.SimpleDirectoryReader-LlamaIndex.https:\n//docs.llamaindex.ai/en/stable/examples/data\nconnectors/\nsimple\ndirectoryreader.html. [Accessed 08-01-2024].\n[33]  Lucidspark.5   Group   Brainstorming   Techniques   for   Winning   Teams   —","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":42,"lines":{"from":1,"to":27}}}}],["doc-81",{"pageContent":"lucidspark.com.https://lucidspark.com/blog/5-group-brainstorming-\ntechniques. [Accessed 08-01-2024].\n[34]  John McCarthy, Marvin L Minsky, Nathaniel Rochester, and Claude E Shannon.  A\nproposal for the dartmouth summer research project on artificial intelligence, august\n31, 1955.AI magazine, 27(4):12–12, 2006.\n[35]  Pradeep  Menon.Introduction  to  Large  Language  Models  and  the  Transformer\nArchitecture.https://rpradeepmenon.medium.com/introduction-to-large-\nlanguage-models-and-the-transformer-architecture-534408ed7e61.   [Ac-\ncessed 08-01-2024].\n[36]  Meta. Llama 2 - Meta AI — ai.meta.com.https://ai.meta.com/llama/. [Accessed\n08-01-2024].\n[37]  Microsoft.Azure  OpenAI  Service  –  Advanced  Language  Models.https://\nazure.microsoft.com/en-us/products/ai-services/openai-service.[Ac-\ncessed 08-01-2024].\n[38]  Prakash M Nadkarni, Lucila Ohno-Machado, and Wendy W Chapman.  Natural lan-\nguage processing: an introduction.Journal of the American Medical Informatics As-\nsociation, 18(5):544–551, 2011.\n[39]  Aakanksha   Patil.Top   5   Pre-trained   Word   Embeddings.https://patil-\naakanksha.medium.com/top-5-pre-trained-word-embeddings-20de114bc26.\n[Accessed 04-01-2024].\n37","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":42,"lines":{"from":28,"to":48}}}}],["doc-82",{"pageContent":"[40]  QA Programmer.  Techniques for Estimating the Time Required for Software Testing\n— linkedin.com.https://www.linkedin.com/pulse/techniques-estimating-\ntime-required-software-testing-qa-programmer/. [Accessed 08-01-2024].\n[41]  Promptingguide.Prompt  Engineering  Guide  —  Prompt  Engineering  Guide  —\npromptingguide.ai.https://www.promptingguide.ai/. [Accessed 04-01-2024].\n[42]  Promptingguide.   Retrieval  Augmented  Generation  (RAG)  —  Prompt  Engineering\nGuide — promptingguide.ai.https://www.promptingguide.ai/techniques/rag.\n[Accessed 04-01-2024].\n[43]  Vineet Raina, Srinath Krishnamurthy, Vineet Raina, and Srinath Krishnamurthy. Nat-\nural language processing.Building an Effective Data Science Practice: A Framework\nto Bootstrap and Manage a Successful Data Science Practice, pages 63–73, 2022.\n[44]  Gourav   Singh.Introduction   to   Artificial   Neural   Networks   —   analyticsvid-\nhya.com.https://www.analyticsvidhya.com/blog/2021/09/introduction-\nto-artificial-neural-networks/. [Accessed 04-01-2024].\n[45]  Jeff Su.  Master the perfect chatgpt prompt formula (in just 8 minutes)!  — youtu.be.\nhttps://youtu.be/jC4v5AS4RIM?si=eAJLgs8iL1IsW-Ov.\n[46]  Deepthi  Sudharsan.Decoding  the  Relationship:   Language  Models  and  Natural\nLanguage  Processing.https://medium.com/@deepthi.sudharsan/decoding-\nthe-relationship-language-models-and-natural-language-processing-\nbbc0cd6754e2. [Accessed 04-01-2024].","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":43,"lines":{"from":1,"to":20}}}}],["doc-83",{"pageContent":"bbc0cd6754e2. [Accessed 04-01-2024].\n[47]  Superannotate.  Fine-tuning large language models (LLMs) in 2023 — SuperAnno-\ntate  —  superannotate.com.https://www.superannotate.com/blog/llm-fine-\ntuning#:\n ̃\n:text=science%20educational%20platform.-,Fine%2Dtuning%\n20methods,its%20ability%20for%20specific%20tasks..[Accessed   07-01-\n2024].\n[48]  MLM   Team.Prompt   Engineering   for   Effective   Interaction   with   ChatGPT\n-    MachineLearningMastery.com    —    machinelearningmastery.com.https:\n//machinelearningmastery.com/prompt-engineering-for-effective-\ninteraction-with-chatgpt/. [Accessed 04-01-2024].\n[49]  Testsigma.Test  Design  in  Software  Testing  -  A  Comprehensive  Guide  —  test-\nsigma.com.https://testsigma.com/blog/test-design/.[Accessed  04-01-\n2024].\n[50]  Solano   Todeschini.How   to   Chunk   Text   Data   -   A   Comparative   Analysis\n— towardsdatascience.com.https://towardsdatascience.com/how-to-chunk-\ntext-data-a-comparative-analysis-3858c4a0997a. [Accessed 08-01-2024].\n[51]  Hoang Tran.  Which is better, retrieval augmentation (RAG) or fine-tuning?  Both. —\nsnorkel.ai.https://snorkel.ai/which-is-better-retrieval-augmentation-\nrag-or-fine-tuning-both/#:\n ̃\n:text=Fine%2Dtuning%20vs.,set%20of%\n20long%2Dterm%20tasks.[Accessed 07-01-2024].\n38","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":43,"lines":{"from":20,"to":44}}}}],["doc-84",{"pageContent":"[52]  Treehouse.   Happy  Path  vs.  Testing  Edge  Cases.https://teamtreehouse.com/\nlibrary/introduction-to-qa-engineering/happy-path-vs-testing-\nedge-cases#:\n ̃\n:text=Definitions%3A,or%20interact%20with%20the%\n20application. [Accessed 08-01-2024].\n[53]  Princeton   University.cs.princeton.edu.https://www.cs.princeton.edu/\ncourses/archive/spring20/cos598C/lectures/lec3-contextualized-\nword-embeddings.pdf. [Accessed 04-01-2024].\n[54]  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Łukasz Kaiser, and Illia Polosukhin.  Attention is all you need.Advances in\nneural information processing systems, 30, 2017.\n[55]  Akanksha Verma,  Amita Khatana,  and Sarika Chaudhary.   A comparative study of\nblack box testing and white box testing.International Journal of Computer Sciences\nand Engineering, 5(12):301–304, 2017.\n[56]  Amol Wagh.  What’s Generative AI? Explore Underlying Layers of Machine Learn-\ning  and  Deep  Learning.https://s.com/@amol-wagh/whats-generative-ai-\nexplore-underlying-layers-of-machine-learning-and-deep-learning-\n8f99272e0b0d. [Accessed 08-01-2024].\n[57]  Wallarm. What is White Box Testing? Types, Techniques, Examples — wallarm.com.\nhttps://www.wallarm.com/what/white-box-testing. [Accessed 08-01-2024].\n[58]  Bin Wang, Angela Wang, Fenxiao Chen, Yuncheng Wang, and C-C Jay Kuo. Evaluat-\ning word embedding models: Methods and experimental results.APSIPA transactions\non signal and information processing, 8:e19, 2019.","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":44,"lines":{"from":1,"to":24}}}}],["doc-85",{"pageContent":"[59]  NSW  Goverment  website.Phases  of  design  thinking  —  education.nsw.gov.au.\nhttps://education.nsw.gov.au/teaching-and-learning/curriculum/\nstem/early-stage-1-to-stage-3/project-based-learning-and-design-\nthinking/phases-of-design-thinking. [Accessed 08-01-2024].\n[60]  Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.  Emergent abil-\nities of large language models.arXiv preprint arXiv:2206.07682, 2022.\n[61]  Susan C Weller.  Structured interviewing and questionnaire construction.Handbook\nof methods in cultural anthropology, pages 365–409, 1998.\n[62]  Janet BW Williams.  A structured interview guide for the hamilton depression rating\nscale.Archives of general psychiatry, 45(8):742–747, 1988.\n[63]  Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan.   Llm lies:\nHallucinations  are  not  bugs,  but  features  as  adversarial  examples.arXiv  preprint\narXiv:2310.01469, 2023.\n[64]  Wayne  Xin  Zhao,  Kun  Zhou,  Junyi  Li,  Tianyi  Tang,  Xiaolei  Wang,  Yupeng  Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.   A survey of large\nlanguage models.arXiv preprint arXiv:2303.18223, 2023.\n39","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":44,"lines":{"from":25,"to":42}}}}],["doc-86",{"pageContent":"A   Interview Questions\nScript start\nWelcome to this short interview, let’s start by saying thank you for taking your time.  The\ninterview will take approximately 20 minutes. The answers from the interview will be used\nas a part of a research project.  None of the answers will be connected to you as a person\nand will only be used in the context of this study.  To participate in the study is voluntary\nand you can decide to stop the interview at any time.  Before starting the interview, do you\nagree with these conditions?\nScript end\nOpen questions for user understanding\nQ1: What is your role at PayPal?\nQ2: How many years of experience do you have in that role and in the company?\nQuestions with Testing Focus\nQ3:  One part of the QA role is to create Test Cases.  How often would you estimate you\ncreate test cases per quarter when being of full capacity?\nQ4:  If we would now focus on the process of creating Test Cases, how does this process\nlook like for you? Please explain the process from start to finish\nQ5: During the process of creating Test Cases do you tend to create these test cases alone or\nin collaboration with another employee? If in collaboration, what roles do these employees\nhave?\nQ6: What would you describe as your biggest challenge when creating Test Cases?\nQ7: Is there any specific part of the process that you would describe as repetitive or annoy-\ning?\nQuestions with AI\nQ8: Have you ever integrated with a Large Language Model such as ChatGTP?","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":45,"lines":{"from":1,"to":25}}}}],["doc-87",{"pageContent":"Q9: Have you ever used such tool to solve a work related task?\nQ10: How often do you find yourself searching this kind of help? Context?\nQ11: Do you know which LLMs have been cleared to access through PayPal and how you\ncan access these?\nQuestions with AI\nHere are some question with a practical exercise:\nIn front of you, you will have ChatGTP. I will give you some instructions and I want you\nto use ChatGTP to get an appropriate answer to what I am asking for.  The answer from\nChatGTP should be as specific and as useful as possible, meaning you should aim to get an\nanswer that does not need to be rewritten.\n40","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":45,"lines":{"from":26,"to":36}}}}],["doc-88",{"pageContent":"Q12: 3 recipes for a blog\nQ13: An analysis of the book A Little Life\nQ14: Test case for a simple login function at Facebook\nQuestions with AI and security/fear\nQ15:  What are your initial thought regarding the usage of AI in the context of Software\ntesting? Do you see any ethical issues that can potentially be faced?\n41","metadata":{"source":"./data/sample.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"","Author":"","Subject":"","Keywords":"","Creator":"LaTeX with hyperref","Producer":"pdfTeX-1.40.24","CreationDate":"D:20240207123056Z","ModDate":"D:20240207123056Z","Trapped":{"name":"False"}},"metadata":null,"totalPages":46},"loc":{"pageNumber":46,"lines":{"from":1,"to":7}}}}]],{"0":"doc-0","1":"doc-1","2":"doc-2","3":"doc-3","4":"doc-4","5":"doc-5","6":"doc-6","7":"doc-7","8":"doc-8","9":"doc-9","10":"doc-10","11":"doc-11","12":"doc-12","13":"doc-13","14":"doc-14","15":"doc-15","16":"doc-16","17":"doc-17","18":"doc-18","19":"doc-19","20":"doc-20","21":"doc-21","22":"doc-22","23":"doc-23","24":"doc-24","25":"doc-25","26":"doc-26","27":"doc-27","28":"doc-28","29":"doc-29","30":"doc-30","31":"doc-31","32":"doc-32","33":"doc-33","34":"doc-34","35":"doc-35","36":"doc-36","37":"doc-37","38":"doc-38","39":"doc-39","40":"doc-40","41":"doc-41","42":"doc-42","43":"doc-43","44":"doc-44","45":"doc-45","46":"doc-46","47":"doc-47","48":"doc-48","49":"doc-49","50":"doc-50","51":"doc-51","52":"doc-52","53":"doc-53","54":"doc-54","55":"doc-55","56":"doc-56","57":"doc-57","58":"doc-58","59":"doc-59","60":"doc-60","61":"doc-61","62":"doc-62","63":"doc-63","64":"doc-64","65":"doc-65","66":"doc-66","67":"doc-67","68":"doc-68","69":"doc-69","70":"doc-70","71":"doc-71","72":"doc-72","73":"doc-73","74":"doc-74","75":"doc-75","76":"doc-76","77":"doc-77","78":"doc-78","79":"doc-79","80":"doc-80","81":"doc-81","82":"doc-82","83":"doc-83","84":"doc-84","85":"doc-85","86":"doc-86","87":"doc-87","88":"doc-88"}]